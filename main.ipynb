{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.models.classification import Classification\n",
    "from src.models.model_selection import grid_search, feature_selector\n",
    "from src.models.utils import get_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config\n",
    "with open('config.yml', 'r') as file:\n",
    "    config=yaml.load(file, Loader= yaml.SafeLoader)\n",
    "del file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(config['data_loader']['path'])\n",
    "display(df.head())\n",
    "\n",
    "# split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.iloc[:,1:-1], df['Class']\n",
    "    , test_size=config['model_selection']['test_set_size']\n",
    "    , random_state=123\n",
    "    , shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(\n",
    "    X=X_train\n",
    "    , y=y_train\n",
    "    , hyper_params=config['model_selection']['algorithms']\n",
    "    , cv=config['model_selection']['cross_validator']\n",
    "    , scoring_metric=config['model_selection']['scoring_metric']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select best algorithm and its hyper-params\n",
    "best_algorithm, best_hyper_params = get_run(\n",
    "    experiment_names=['model_evaluation']\n",
    "    , order_by_metric=True\n",
    "    , metric_name=config['model_selection']['scoring_metric']\n",
    "    , num=2\n",
    ")\n",
    "\n",
    "# greedy feature selection\n",
    "\"\"\"best_features = feature_selector(\n",
    "    X=X_train\n",
    "    , y=y_train\n",
    "    , algorithm=best_algorithm\n",
    "    , algorithm_params=best_hyper_params\n",
    "    , tol=config['model_selection']['tolerance']\n",
    "    , cv=config['model_selection']['cross_validator']\n",
    "    , scoring_metric=config['model_selection']['scoring_metric']\n",
    ")\"\"\"\n",
    "\n",
    "print(\n",
    "    f'algorithm: {best_algorithm}'\n",
    "    , f'\\nparameters: {best_hyper_params}'\n",
    "    #, f'\\nfeatures: {best_features}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit best algorithm on most important features of training data \n",
    "clf = Classification(algorithm=best_algorithm, **best_hyper_params)\n",
    "clf.fit(X=X_train.iloc[:,[3,6,8]], y=y_train)\n",
    "# predict target value for the test set\n",
    "y_pred = clf.predict(X_test.iloc[:,[3,6,8]])\n",
    "y_score = clf.score(X_test.iloc[:,[3,6,8]])[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visuals.boundary import plot_boundary\n",
    "\n",
    "plot_boundary(\n",
    "    X=X_test.iloc[:,[3,6,8]], y=y_test, clf=clf, azim=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({'score': y_score, 'label': y_pred}).groupby(by=['label']).describe()\n",
    "#print(clf.model.decision_path(X_test[best_features[:2]].iloc[:10,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, auc, RocCurveDisplay,\n",
    "    precision_recall_curve, PrecisionRecallDisplay, average_precision_score\n",
    ")\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\n",
    "    f\"\"\"Accuracy : {round(accuracy_score(y_test, y_pred), 5)}\n",
    "Precision: {round(precision_score(y_test, y_pred), 5)}\n",
    "Recall   : {round(recall_score(y_test, y_pred), 5)}\n",
    "F1-Score : {round(f1_score(y_test, y_pred), 5)}\"\"\"\n",
    ")\n",
    "\n",
    "# plot ROC and PR curves\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "# plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "ax[0].plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC Curve (AUC = {roc_auc:.3f})\")\n",
    "ax[0].plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\", lw=2, label=\"Random Guessing\")\n",
    "ax[0].set_xlabel(\"False Positive Rate\")\n",
    "ax[0].set_ylabel(\"True Positive Rate\")\n",
    "ax[0].set_title(\"ROC Curve\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "# plot PR curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "ap_score = average_precision_score(y_test, y_score)\n",
    "ax[1].plot(recall, precision, color=\"green\", lw=2, label=f\"PR Curve (AP = {ap_score:.3f})\")\n",
    "ax[1].set_xlabel(\"Recall\")\n",
    "ax[1].set_ylabel(\"Precision\")\n",
    "ax[1].set_title(\"Precision-Recall Curve\")\n",
    "ax[1].legend(loc=\"lower left\")\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
